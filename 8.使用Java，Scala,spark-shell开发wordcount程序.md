[toc]
# 1.用Java开发wordcount程序
## 1.1配置maven环境
```xml
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
  <modelVersion>4.0.0</modelVersion>

  <groupId>cn.spark</groupId>
  <artifactId>spark-study-java</artifactId>
  <version>0.0.1-SNAPSHOT</version>
  <packaging>jar</packaging>

  <name>spark-study-java</name>
  <url>http://maven.apache.org</url>

  <properties>
    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
  </properties>

  <dependencies>
    <dependency>
      <groupId>junit</groupId>
      <artifactId>junit</artifactId>
      <version>3.8.1</version>
      <scope>test</scope>
    </dependency>
    <dependency>
    	<groupId>org.apache.spark</groupId>
    	<artifactId>spark-core_2.10</artifactId>
    	<version>1.3.0</version>
	</dependency>
	<dependency>
    	<groupId>org.apache.spark</groupId>
    	<artifactId>spark-sql_2.10</artifactId>
    	<version>1.3.0</version>
	</dependency>
	<dependency>
    	<groupId>org.apache.spark</groupId>
    	<artifactId>spark-hive_2.10</artifactId>
    	<version>1.3.0</version>
    	<scope>provided</scope>
	</dependency>
	<dependency>
    	<groupId>org.apache.spark</groupId>
    	<artifactId>spark-streaming_2.10</artifactId>
    	<version>1.3.0</version>
    	<scope>provided</scope>
	</dependency>
	<dependency>
	    <groupId>org.apache.hadoop</groupId>
	    <artifactId>hadoop-client</artifactId>
	    <version>2.4.1</version>
	</dependency>
	<dependency>
	    <groupId>org.apache.spark</groupId>
	    <artifactId>spark-streaming-kafka_2.10</artifactId>
	    <version>1.3.0</version>
	</dependency>
	
  </dependencies>


	
	<build>
		<sourceDirectory>src/main/scala</sourceDirectory>
    	<testSourceDirectory>src/test/scala</testSourceDirectory>
    	<plugins>
      		<plugin>
      			<artifactId>maven-assembly-plugin</artifactId>
      			<configuration>
      				<descriptorRefs>
      					<descriptorRef>jar-with-dependencies</descriptorRef>
      				</descriptorRefs>
      				<archive>
      					<manifest>
      						<mainClass></mainClass>
      					</manifest>
      				</archive>
      			</configuration>
      			<executions>
      				<execution>
      					<id>make-assembly</id>
      					<phase>package</phase>
      					<goals>
      						<goal>single</goal>
      					</goals>
      				</execution>
      			</executions>
      		</plugin>
	        <plugin>
	            <groupId>org.codehaus.mojo</groupId>
	            <artifactId>exec-maven-plugin</artifactId>
	            <version>1.2.1</version>
	            <executions>
	                <execution>
	                    <goals>
	                        <goal>exec</goal>
	                    </goals>
	                </execution>
	            </executions>
	            <configuration>
	                <executable>java</executable>
	                <includeProjectDependencies>true</includeProjectDependencies>
	                <includePluginDependencies>false</includePluginDependencies>
	                <classpathScope>compile</classpathScope>
	                <mainClass>cn.spark.study.App</mainClass>
	            </configuration>
	        </plugin>
	        <plugin>
	            <groupId>org.apache.maven.plugins</groupId>
	            <artifactId>maven-compiler-plugin</artifactId>	
	            <configuration>
	                <source>1.6</source>
	                <target>1.6</target>
	            </configuration>
        	</plugin>   		
      	</plugins>	
	</build>

</project>
```
## 1.2如何进行本地测试
```scala
package cn.spark.study.core;

import java.util.Arrays;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.api.java.function.PairFunction;
import org.apache.spark.api.java.function.VoidFunction;

import scala.Tuple2;

/*
  * 本地测试程序
 * 
 */

public class WordCountLocal {
	public static void main(String[] args) {
	//	编写spark应用程序
	//第一步：创建sparkConf对象，设置Spark应用的配置信息
	//使用setMaster()可以设置spark应用程序要连接的spark集群的master节点的url
	//但是如果设置为local则代表在本地运行
		SparkConf conf = new SparkConf()
				.setAppName("WordCountLocal")
				.setMaster("local");
	//第二步:创建JavaSparkContext对象
		//在spark中,SparkContext是spark所有功能的一个入口,你无论是用Java、scala,甚至是python编写
		//都必须要有一个SparkContext,它的主要作用,包括初始化spark应用程序所需的一些核心组件,包括
		//调度器(DAGSchedule、TaskScheduler),还会去到Spark Master节点上进行注册,等等

		//一句话,SparkContext,是spark应用中,可以说是最最重要的一个对象

		//但是呢,在spark中,编写不同类型的spark应用程序,使用的SparkContext是不同的
		//如果使用scala,使用的就是原生的SparkContext对象
		//但是如果使用Java,那么就是JavaSparkContext对象
		//如果是开发spark SQL程序,那么就是SQLContext、HiveContext
		//如果是开发spark Streaming程序,那么就是他独有的SparkContext
		
		JavaSparkContext sc = new JavaSparkContext(conf);
	//第三步要针对输入源（hdfs文件，本地文件等等），创建一个初始的RDD
		//输入源中的数据会打散,分配到RDD的每个partition中,从而形成一个初始的分布式的数据集
		//SparkContext中,用于根据文件类型的输入源创建RDD的方法,叫做textFile()方法
		//在Java中，创建的普通RDD，都叫做JavaRDD
		//如果是hdfs或者本地文件创建的RDD，每个元素相当文件的一行，也就是元素概念
		
		JavaRDD<String> lines = sc.textFile("spark.txt");
	//第四步：对初始化RDD进行transformation操作，也就是一些计算操作
		//通常操作会通过创建function,并配合RDD的map、flatmap等算子来执行
		//function,通常,如果比较简单,则创建指定function的匿名内部类
		//但是如果function比较复杂,则会单独创建一个类，作为实现这个function接口的类		
		
		//将每行拆成一个单词
		//FlatMapFunction有两个泛型参数，分别是输入和输出类型
		//flatmap算子的作用,其实就是,将RDD的一个元素,给拆分成一个或多个元素

	
		JavaRDD<String> words =lines.flatMap(new FlatMapFunction<String, String>() {

			/**
			 * 
			 */
			private static final long serialVersionUID = 1L;

			@Override
			public Iterable<String> call(String line) throws Exception {
				// TODO Auto-generated method stub
				return Arrays.asList(line.split(" "));
			}
		});
		//映射为（单词，1）
		//mapToPair 就是将其映射为（k,v）,Tuple2类型。这里的Tuple2就是scala类型
		//mapToPair要与PairFunction匹配，第一泛型是输入，第二个和第三个是输出泛型，Tuple2
		//JavaPairRDD两个泛型参数，对应Tuple2
		JavaPairRDD<String, Integer> pairs = words.mapToPair(new PairFunction<String, String, Integer>() {

			/**
			 * 
			 */
			private static final long serialVersionUID = 1L;

			@Override
			public Tuple2<String, Integer> call(String word) throws Exception {
				// TODO Auto-generated method stub
				return new Tuple2<String, Integer>(word, 1);
			}
			
		});
		//接着，以单词为key，统计出现的次数
		//使用reduceByKey算子，对每个key对应的value进行reduce操作
		JavaPairRDD<String, Integer> wordCount=pairs.reduceByKey(new Function2<Integer, Integer, Integer>() {
			
			/**
			 * 
			 */
			private static final long serialVersionUID = 1L;

			@Override
			public Integer call(Integer v1, Integer v2) throws Exception {
				// TODO Auto-generated method stub
				return v1+v2;
			}
		});
		//到此为止，通过几个spark算子操作，计算出单词出现次数		
		//之前是我们使用的flatMap，mapToPair,reduceByKey操作，都叫做transformation操作
		//transformation和action组合，才能运行spark应用
		//接着，action操作，比如foreach来触发程序执行
		wordCount.foreach(new VoidFunction<Tuple2<String,Integer>>() {
			
			/**
			 * 
			 */
			private static final long serialVersionUID = 1L;

			@Override
			public void call(Tuple2<String, Integer> wordCount) throws Exception {
				// TODO Auto-generated method stub
				System.out.println(wordCount._1+":"+wordCount._2);
			}
		});
		sc.close();
	}
}
```
## 1.3如何使用spark-submit提交到spark集群进行执行
(spark-submi常用参数说明,spark-submit其实就类似于hadoop的hadoop.jar命令)
1.将spark.txt上传到spark1的：/usr/local/下
 hadoop fs -put spark.txt /spark.txt
 查看方式：浏览器上spark1:50070下utilities-》Browse the file system
 2.根据pom.xml里的配置maven插件，对spark进行打包
 打包过程：右键项目-》run as-》run configurations-》Maven Build右击-》new-》Name:(起名字，如spark-study-java)->点击
 browse workspace选择项目-》Goals输入clean package-Run
 最后生成spark-study-java-0.0.1-SNAPSHOT-jar-with-dependencies.jar
3. 将jar包上传到spark集群上，（我的位置root@spark1:/usr/local/spark-study/java/）
4. 编写spark-submit脚本，然后执行脚本，提交到集群上
wordcount.sh
```sh
/usr/local/spark/bin/spark-submit \
--class cn.spark.study.core.WorkCountCluster \
--num-executors 3 \
--driver-memory 100m \
--executor-memory 100m \
--executor-cores 3 \
/usr/local/spark-study/java/spark-study-java-0.0.1-SNAPSHOT-jar-with-dependencies.jar \
```
给sh文件权限，chmod 777 wordcount.sh
5.执行./wordcount.sh
浏览器可以查看（运行时可以看）http://spark1:4040

# 2.用Scala开发wordcount程序
## 2.1下载scala ide for eclipse
## 2.2在Java Build Path中,添加spark依赖包
(如果与scala ide for eclpse原生的scala版本
发生冲突,则移除原生的scala,重新配置scala compiler)
* 在spark安装文件spark-1.3.0-bin-hadoop2.4.tgz中，目录下lib下的
spark-assembly-1.3.0-hadoop2.4.0.jar添加到eclipse  libraries
* 建立包cn.spark.study.core,创建类WordCount

``` scala
package cn.spark.study.core

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

object WordCount {
  def main(args: Array[String]): Unit = {
    val conf=new SparkConf().setAppName("WordCount")
    val sc = new SparkContext(conf)
    
    val lines=sc.textFile("hdfs://spark1:9000/spark.text", 1)
    val words = lines.flatMap { line => line.split(" ") }
    val pairs=words.map{word=>(word,1)}
    val wordcounts = pairs.reduceByKey{_+_}
    
    wordcounts.foreach(wordcount=>{println(wordcount._1+":"+wordcount._2)})
  }
}
```
一些坑：点击项目然后右键选中properties=》scala compiler=>scala installation选中Latest 2.10 bundle  后面jvm-1.6
## 2.3用添加export导出scala spark工程
将导出的jar包上传spark集群，然后执行其脚本


# 3.用spark-shell开发wordcount程序
## 3.1常用于简单的测试

# 4.wordcount程序原理剖析
![8-1 wordcount程序原理剖析图](https://github.com/2415970940/save_img/raw/master/spark/8/8-1.png)

# 5.spark架构原理
1.Drive
2.Master
3.Worker
4.Executor
5.Task

![8-2 spark构架原理](https://github.com/2415970940/save_img/raw/master/spark/8/8-2.jpg)

![8-3 spark执行顺序](https://github.com/2415970940/save_img/raw/master/spark/8/8-3.jpg)

# 6.Spark编程创建RDD
## 创建RDD

进行Spark核心编程时,首先要做的第一件事,就是创建一个初始的RDD。该RDD中,通常就代表和包含了Spark应用程序的输入源数据。然后在创建了初始的RDD之后,才可以通过Spark Core提供的transformation算子,对该RDD进行转换,来获取其他的RDD。

SparkCore提供了三种创建RDD的方式,包括:使用程序中的集合创建RDD；使用本地文件创建RDD;使用HDFS文件创建RDD。

个人经验认为:

1、使用程序中的集合创建RDD,主要用于进行测试,可以在实际部署到集群运行之前,自己使用集合构造测试数据,来测试后面的spark应用的流程。

2、使用本地文件创建RDD,主要用于临时性地处理一些存储了大量数据的文件。

3、使用HDFS文件创建RDD,应该是最常用的生产环境处理方式,主要可以针对HDFS上存储的大数据,进行离线批处理操作。
### 并行化集合创建RDD
如果要通过并行化集合来创建RDD,需要针对程序中的集合,调用SparkContext的parallelize()方法。Spark会将集合中的数据拷贝到集群上去,形成一个分布式的数据集合,也就是一个RDD。相当于是,集合中的部分数据会到一个节点上,而另一部分数据会到其他节点上。然后就可以用并行的方式来操作这个分布式数据集合,即RDD
```scala
val arr=Array(1,2,3,4,5,6,7,8,9,10)
val rdd=sc.parallelize(arr)
val sum=rdd.reduce(_+_)
```
调用parallelize()时,有一个重要的参数可以指定,就是要将集合切分成多少个partition.Spark会为每一个partition运行一个task来进行处理。Spark官方的建议是,为集群中的每个CPU创建2~4个partition。Spark默认会根据集群的情况来设置partition的数量。但是也可以在调用parallelize()方法时,传入第二个参数,来设置RDD的partition数量。比如parallelize(arr,10)

``` java
package cn.spark.study.core;

import java.util.*;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function2;

import antlr.collections.List;

/*
 * 
  * 并行化集合创建
 */
public class ParallelizeCollection {
	public static void main(String[] args) {
		//创建SparkCong
		SparkConf conf = new SparkConf()
				.setAppName("ParallelizeCollection")
				.setMaster("local");
		//创建SparkContext
		JavaSparkContext sc=new JavaSparkContext(conf);
		//通过并行化集合的方式创建RDD，调用SparkContext及其子类的parallelize方法
		java.util.List<Integer> numbers=Arrays.asList(1,2,3,4,5,6,7,8,9,10);
		JavaRDD<Integer> numbersRDD = sc.parallelize(numbers);
		//执行reduce算子操作，(((1+2)+3)+4)+5...
		int sum=numbersRDD.reduce(new Function2<Integer, Integer, Integer>() {
			
			/**
			 * 
			 */
			private static final long serialVersionUID = 1L;

			@Override
			public Integer call(Integer v1, Integer v2) throws Exception {
				// TODO Auto-generated method stub
				return v1+v2;
			}
		});
		System.out.println("累加1到10："+sum);
		
		sc.stop();		
	}
}
```

```scala
package cn.spark.study.core

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

object ParallelizeCollection {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("ParallelizeCollection").setMaster("local")
    val sc = new SparkContext(conf)
    
    val numbers=Array(1,2,3,4,5,6,7,8,9)
    val numberRDD = sc.parallelize(numbers, 5)
    val sum = numberRDD.reduce(_+_)
    
    println(sum)  
  }
}
```
### 使用本地文件和HDFS创建RDD
Spark是支持使用任何Hadoop支持的存储系统上的文件创建RDD的,比如说HDFS、Cassandra、HBase以及本地文件。通过调用SparkContext的textFile（）方法,可以针对本地文件或HDFS文件创建RDD。

有几个事顶是需要注意的:

1、如果是针对本地文件的话,如果是在windows上本地测试,windows上有一份文件即可;如果是在spark集群上针对linux本地文件,那么需要将文件拷贝到所有worker节点上。

2、Spark的textFile（）方法支持针对目录、压缩文件以及通配符进行RDD创建。

3、Spark默认会为hdfs文件的每一个block创建一个partiton,但是也可以通过textFile（）的第二个参数手动设置分区数里,只能比block数量多,不能比block数量少。
```scala
val rdd=sc.textFile("data.txt")
val wordCount=rdd.map(line=>line.length).reduce(_+_)
```
# 这里本地文件和HDFS创建RDD代码暂时省略，标注一下

Spark的textFile()除了可以针对上述几种普通的文件创建RDD之外,还有一些特殊的方法来
创建RDD:

1、SparkContext.wholeTextFiles()方法,可以针对一个目录中的大量小文件,返回<filename,fileContent>组成的pair,作为一个PairRDD,而不是普通的RDD。普通的textFile()返回的RDD中,每个元素就是文件中的一行文本。
2、SparkContext.sequenceFile[K,V]()方法,可以针对SequenceFile创建RDD,K和V泛型类型就是SequenceFile的key和value的类型。K和V要求必须是Hadoop的序列化类型,比如IntWritable、Text等。

3、SparkContext.hadoopRDD()方法,对于Hadoop的自定义输入类型,可以创建RDD。该方法接收JobConf、InputFormatClass、Key和Value的Class.
4、SparkContext.objectFile()方法,可以针对之前调用RDD.saveAsObjectFile()创建的对象序列化的文件,反序列化文件中的数据,并创建一个RDD。

# RDD操作
## transformation和action介绍

Spark支持两种RDD操作,transformation和action。transformation操作会针对已有的RDD创建一个新的RDD;而action则主要是对RDD进行最后的操作,比如遍历、reduce、保存到文件等,并可以返回结果给Driver程序。

例如,map就是一种transformation操作,它用于将已有RDD的每个元素传入一个自定义的函数,并获取一个新的元素,然后将所有的新元素组成一个新的RDD。而reduce就是一种action操作,它用于对RDD中的所有元素进行聚合操作,并获取一个最终的结果,然后返回给Driver程序。

transformation的特点就是lazy特性。lazy特性指的是,如果一个spark应用中只定义了transformation操作,那么即使你执行该应用,这些操作也不会执行。也就是说,transformation是不会触发spark程序的执行的,它们只是记录了对RDD所做的操作,但是不会自发的执行。只有当transformation之后,接着执行了一个action操作,那么所有的transformation才会执行。Spark通过这种lazy特性,来进行底层的spark应用执行的优化,避免产生过多中间结果。

action操作执行,会触发一个spark job的运行,从而触发这个action之前所有的transformation的执行。这是action的特性。
![8-4 transformation和action原理剖析]()

## 案例：统计文件字数
这里通过一个之前学习过的案例,统计文件字数,来讲解transformation和action。

这里通过textFile()方法,针对外部文件创建了一个RDD,lines,但是实际上,程序执行到这里为止,Spark.txt文件的数据是不会加载到内存中的。lines,只是代表了一个指向sparktxt文件的引用。
val lines=sc.textFile("'spark.txt")

这里对lines RDD进行了map算子,获取了一个转换后的lineLengths RDD。但是这里连数据都没有,当然也不会做任何操作。lineLengths RDD也只是一个概念上的东西而已。
val lineLengths=lines.mapl(line=>line.length)

之列,执行了一个action操作,reduce。此时就会触发之前所有transformation操作的执行,Spark会将操作拆分成多个task到多个机器上并行执行,每个task会在本地执行map操作,并且进行本地的reduce聚合。最后会进行一个全局的reduce聚合,然后将结果返回给Driver程序。

val totalLength=lineLengths.reduce(_+_)

## 案例：统计文件每行出现的次数

Spark有些特殊的算子,也就是特殊的transformation的操作，比如groupByKey、sortByKey、reduceByKey等,其实只是针对特殊的RDD的。即包含key-value对的RDD。而这种RDD中的元素,实际上是scala中的一种类型,即Tuple2,也就是包含两个值的Tuple

在scala中,需要手动导入Spark的相关隐式转换,import.org.apache.spark.SparkContext._。然后,对应包含Tuple2的RDD,会自动隐式转换为PairRDDFunction,并提供reduceByKey等方法。

val lines=sc.textFile("hello.txt")

val linePairs=lines.map｛line=>(line,1)｝

val lineCounts=linePairs.reduceByKey(_+_)

lineCounts.foreach(lineCount=>printin(lineCount_1+"appears"+llineCount_2+"times."))

``` java
package cn.spark.study.core;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.api.java.function.PairFunction;
import org.apache.spark.api.java.function.VoidFunction;

import scala.Tuple2;

public class LineCount {
	public static void main(String[] args) {
		SparkConf conf= new SparkConf()
				.setAppName("LineCount")
				.setMaster("local");
		JavaSparkContext sc=new JavaSparkContext(conf);
		
		JavaRDD<String> lines = sc.textFile("H://spark//spark-study//hello.txt");
		JavaPairRDD<String, Integer> linePairs=lines.mapToPair(new PairFunction<String, String, Integer>() {

			@Override
			public Tuple2<String, Integer> call(String line) throws Exception {
				// TODO Auto-generated method stub
				return new Tuple2<String, Integer>(line, 1);
			}
		});
		
		JavaPairRDD<String, Integer> lineCounts=linePairs.reduceByKey(new Function2<Integer, Integer, Integer>() {
			
			/**
			 * 
			 */
			private static final long serialVersionUID = 1L;

			@Override
			public Integer call(Integer v1, Integer v2) throws Exception {
				// TODO Auto-generated method stub
				return v1+v2;
			}
		});
		lineCounts.foreach(new VoidFunction<Tuple2<String,Integer>>() {
			
			/**
			 * 
			 */
			private static final long serialVersionUID = 1L;

			@Override
			public void call(Tuple2<String, Integer> lineCount) throws Exception {
				// TODO Auto-generated method stub
				System.out.println(lineCount._1+" appears "+lineCount._2+" times");
			}
		});
		sc.close();
	}
}
```

```scala
package cn.spark.study.core

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

object LineCount {
  def main(args: Array[String]): Unit = {
    val conf=new SparkConf().setAppName("LineCount").setMaster("local")
    val sc=new SparkContext(conf)
    
    val lines=sc.textFile("H://spark//spark-study//hello.txt", 1)
    val linePairs=lines.map(line=>(line,1))
    val lineCounts=linePairs.reduceByKey(_+_)
    
    lineCounts.foreach(lineCount=>println(lineCount._1+" appears "+lineCount._2+" times"))
  }
}
```
