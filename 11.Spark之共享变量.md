[toc]
# 共享变量工作原理
 
Spark一个非常重要的特性就是共享变量。

默认情况下,如果在一个算子的函数使用到了某个外部的变量,那么这个变量的值会被拷贝到每个task中。此时每个task只能操作自己的那份变量副本。如果多个task想要共享某个变量,那么这种方式是做不到的。

Spark为此提供了两种共享变量,一种是Broadcast Variable(广播变量),另一种是Accumulator(累加变量)。Broadcast Variable会将使用到的变量,仅仅为每个节点拷贝一份,更大的用处是优化性能,减少网络传输以及内存消耗。
Accumulator则可以让多个task共同操作一份变量,主要可以进行累加操作。
![11-1 共享变量与非共享变量]()

# Broadcast Variable
Spark提供的BroadcastVariable,是只读的。并且在每个节点上只会有一份副本,而不会为每个task都拷贝一份副本。因此其最大作用,就是减少变量到各个节点的网络传输消耗,以及在各个节点上的内存消耗。此外,spark自己内部也使用了高效的广播算法来减少网络消耗。

可以通过调用SparkContext的broadcast()方法,来针对某个变量创建广播变量。然后在算子的函数内,使用到广播变量时,每个节点只会拷贝一份副本了。每个节点可以使用广播变量的value()方法获取值。记住,广播变量,是只读的。

val factor=3
val factorBroadcast=sc.broadcast(factor)

val arr=Array(1,2,3,4,5)
val rdd=sc.parallelize(arr)
val multipleRdd=rdd.map(num=>num*factorBroadcast.value())

multipleRdd.foreach(num=>printIn(num))

``` java
package cn.spark.study.core;

import java.util.*;
import java.util.List;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.VoidFunction;
import org.apache.spark.broadcast.Broadcast;


/*
 * 广播变量
 */
public class BroadcastVariable {

	public static void main(String[] args) {
		SparkConf conf = new SparkConf()
				.setAppName("BroadcastVariable")
				.setMaster("local");
		JavaSparkContext sc = new JavaSparkContext(conf);
		//在java中创建共享变量，就是调用JavaSparkContext的broadcast方法
		//获取的返回值是Broadcast<T>类型
		int factor=3;
		Broadcast<Integer> factorBroadcast=sc.broadcast(factor);
		List<Integer> numbersList=Arrays.asList(1,2,3,4,5);
		JavaRDD<Integer> numbersRdd=sc.parallelize(numbersList);
		//让每个元素乘以factor
		JavaRDD<Integer> numbers = numbersRdd.map(new Function<Integer, Integer>() {

			@Override
			public Integer call(Integer v1) throws Exception {
				// 使用共享变量，调用其value()方法，获取内部封装的值
				return v1*factorBroadcast.value();
			}
		});
		
		numbers.foreach(new VoidFunction<Integer>() {
			
			@Override
			public void call(Integer t) throws Exception {
				// TODO Auto-generated method stub
				System.out.println(t);
				
			}
		});
		
		sc.close();

	}

}
```
scala

``` scala
package cn.spark.study.core

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext


object BroadcastVariable {
  def main(args: Array[String]): Unit = {
    val conf=new SparkConf().setAppName("BroadcastVariable").setMaster("local")
    val sc=new SparkContext(conf)
    
    val numbersList=Array(1,2,3,4,5)
    val numbers = sc.parallelize(numbersList, 1)
    
    val factor=5
    val factorBroadcast = sc.broadcast(factor)
    
    val numbersrrd=numbers.map(num=>num*factorBroadcast.value)
    
    numbersrrd.foreach { num => println(num) }
  }
}
```

# accumulator

Spark提供的Accumulator,主要用于多个节点对一个变量进行共享性的操作。Accumulator只提供了累加的功能。但是确给我们提供了多个task对一个变量并行操作的功能。但是task只能对Accumulator进行累加操作,不能读取它的值。只有Driver程序可以读Accumulator的值。

val SumAccumulator=sc.accumulator(0)
val arr=Array(1,2,3,4,5)
val rdd=sc.parallelize(arr)
rdd.foreach(num=>SumAccumulator+=num)
println(sumAccumulator.value)

```java
package cn.spark.study.core;

import java.util.Arrays;
import java.util.List;

import org.apache.spark.Accumulator;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.VoidFunction;

public class AccumulatorVariable {

	public static void main(String[] args) {
		SparkConf conf = new SparkConf()
				.setAppName("AccumulatorVariable")
				.setMaster("local");
		JavaSparkContext sc = new JavaSparkContext(conf);
		//创建Accumulator，就是调用JavaSparkContext的accumulator方法
		Accumulator<Integer> sum = sc.accumulator(0);
		
		List<Integer> numbersList=Arrays.asList(1,2,3,4,5);
		JavaRDD<Integer> numbersRdd=sc.parallelize(numbersList);
		
		numbersRdd.foreach(new VoidFunction<Integer>() {
			
			@Override
			public void call(Integer t) throws Exception {
				//在函数内部，对accumulator变量调用add（）方法，累加值
				sum.add(t);
			}
		});
		System.out.println(sum.value());
		
		sc.close();
	}

}

```
scala
```scala
package cn.spark.study.core

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

object AccumulatorVariable {
  def main(args: Array[String]): Unit = {
    val conf=new SparkConf().setAppName("AccumulatorVariable").setMaster("local")
    val sc=new SparkContext(conf)
    
    val numbersList=Array(1,2,3,4,5)
    val numbers = sc.parallelize(numbersList, 1)
    
    val sum=sc.accumulator(0)
    
    numbers.foreach { num => sum +=num }
    
    println(sum)    
  }
}
```

