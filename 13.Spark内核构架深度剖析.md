---
title: 13.Spark内核构架深度剖析
weblog_mt_keywords: "标签1,标签2"
---
[toc]

# spark核心架构


1、Application
2、spark-submit
3、Driver
4、SparkContext
5、Master
6、Worker
7、Executor
8、Job
9、DAGScheduler
10、TaskScheduler
11、ShufieMap Task and ResultTask

![13-1 Spark核心架构](https://github.com/2415970940/save_img/raw/master/spark/13/13-1.jpg)

![13-2 Spark核心架构流程](https://github.com/2415970940/save_img/raw/master/spark/13/13-2.png)
对于13-2图流程详解
Spark任务简介：
Spark-submit—>SparkSubmit–>main–>submit–>doRunMain–>RunMain–>通过反射创建我们编写的主类的实例对象，调用main方法–>开始执行我们编写的代码–>初始化SparkContext对象–>创建初始的RDD–>触发action算子–>提交job–>worker执行任务–>任务结束

Spark任务详解：
1）、将我们编写的程序打成jar包

2）、调用spark-submit脚本提交任务到集群上运行

3）、运行sparkSubmit的main方法，在这个方法中通过反射的方式创建我们编写的主类的实例对象，然后调用main方法，开始执行我们的代码（注意，我们的spark程序中的driver就运行在sparkSubmit进程中）

4）、当代码运行到创建SparkContext对象时，那就开始初始化SparkContext对象了

5）、在初始化SparkContext对象的时候，会创建两个特别重要的对象，分别是：DAGScheduler
和TaskScheduler

【DAGScheduler的作用】将RDD的依赖切分成一个一个的stage，然后将stage作为taskSet提交给DriverActor

6）、在构建taskScheduler的同时，会创建两个非常重要的对象，分别是DriverActor和ClientActor

【clientActor的作用】向master注册用户提交的任务
【DriverActor的作用】接受executor的反向注册，将任务提交给executor

7）、当clientActor启动后，会将用户提交的任务和相关的参数封装到ApplicationDescription对象中，然后提交给master进行任务的注册

8）、当master接受到clientActor提交的任务请求时，会将请求参数进行解析，并封装成Application，然后将其持久化，然后将其加入到任务队列waitingApps中

9）、当轮到我们提交的任务运行时，就开始调用schedule()，进行任务资源的调度

10）、master将调度好的资源封装到launchExecutor中发送给指定的worker

11）、worker接受到Maseter发送来的launchExecutor时，会将其解压并封装到ExecutorRunner中，然后调用这个对象的start(), 启动Executor

12）、Executor启动后会向DriverActor进行反向注册

13）、driverActor会发送注册成功的消息给Executor

14）、Executor接受到DriverActor注册成功的消息后会创建一个线程池，用于执行DriverActor发送过来的task任务

15）、当属于这个任务的所有的Executor启动并反向注册成功后，就意味着运行这个任务的环境已经准备好了，driver会结束SparkContext对象的初始化，也就意味着new SparkContext这句代码运行完成

16）、当初始化sc成功后，driver端就会继续运行我们编写的代码，然后开始创建初始的RDD，然后进行一系列转换操作，当遇到一个action算子时，也就意味着触发了一个job

17）、driver会将这个job提交给DAGScheduler

18）、DAGScheduler将接受到的job，从最后一个算子向前推导，将DAG依据宽依赖划分成一个一个的stage，然后将stage封装成taskSet，并将taskSet中的task提交给DriverActor

19）、DriverActor接受到DAGScheduler发送过来的task，会拿到一个序列化器，对task进行序列化，然后将序列化好的task封装到launchTask中，然后将launchTask发送给指定的Executor

20）、Executor接受到了DriverActor发送过来的launchTask时，会拿到一个反序列化器，对launchTask进行反序列化，封装到TaskRunner中，然后从Executor这个线程池中获取一个线程，将反序列化好的任务中的算子作用在RDD对应的分区上

# 宽依赖与窄依赖

* 窄依赖是指父RDD的每个分区只被子RDD的一个分区所使用，子RDD分区通常对应常数个父RDD分区(O(1)，与数据规模无关) 相应的，
* 宽依赖是指父RDD的每个分区都可能被多个子RDD分区所使用，子RDD分区通常对应所有的父RDD分区(O(n)，与数据规模有关

宽依赖和窄依赖如下图所示：

![13-3 窄依赖与宽依赖](https://github.com/2415970940/save_img/raw/master/spark/13/13-3.png)

相比于宽依赖，窄依赖对优化很有利 ，主要基于以下两点：
- 1.宽依赖往往对应着shuffle操作，需要在运行过程中将同一个父RDD的分区传入到不同的子RDD分区中，中间可能涉及多个节点之间的数据传输；而窄依赖的每个父RDD的分区只会传入到一个子RDD分区中，通常可以在一个节点内完成转换。 
- 2.当RDD分区丢失时（某个节点故障），spark会对数据进行重算。
 	- 对于窄依赖，由于父RDD的一个分区只对应一个子RDD分区，这样只需要重算和子RDD分区对应的父RDD分区即可，所以这个重算对数据的利用率是100%的； 
	- 对于宽依赖，重算的父RDD分区对应多个子RDD分区，这样实际上父RDD 中只有一部分的数据是被用于恢复这个丢失的子RDD分区的，另一部分对应子RDD的其它未丢失分区，这就造成了多余的计算；更一般的，宽依赖中子RDD分区通常来自多个父RDD分区，极端情况下，所有的父RDD分区都要进行重新计算。 
	- 如下图所示，b1分区丢失，则需要重新计算a1,a2和a3，这就产生了冗余计算(a1,a2,a3中对应b2的数据)。
![13-4](https://github.com/2415970940/save_img/raw/master/spark/13/13-4.png)
窄依赖的函数有：map, filter, union, join(父RDD是hash-partitioned ), mapPartitions, mapValues
宽依赖的函数有：groupByKey, join(父RDD不是hash-partitioned ), partitionBy

![13-5 窄依赖与宽依赖深入剖析](https://github.com/2415970940/save_img/raw/master/spark/13/13-5.png)

# Spark提交模式
1、Spark内核架构,其实就是第一种模式,standalone模式,基于Spark自己的Master-Worker集群。

2、第二种,是基于YARN的yarn-cluster模式。

3、第三种,是基于YARN的yarn-client模式。

4、如果,你要切换到第二种和第三种模式,很简单,将我们之前用于提交spark应用程序的spark-submit脚本,加上--master参数,设置为yarn-cluster,或yarn-client,即可。如果你没设置,那么,就是standalone模式。
## Spark Standalone
 Standalone模式是Spark实现的资源调度框架，其主要的节点有Client节点、Master节点和Worker节点。其中Driver既可以运行在Master节点上中，也可以运行在本地Client端。当用spark-shell交互式工具提交Spark的Job时，Driver在Master节点上运行；当使用spark-submit工具提交Job或者在Eclips、IDEA等开发平台上使用”new SparkConf.setManager(“spark://master:7077”)”方式运行Spark任务时，Driver是运行在本地Client端上的。
 
![13-6 standalone示意图](https://github.com/2415970940/save_img/raw/master/spark/13/13-6.png)

其运行过程如下：

*    1.SparkContext连接到Master，向Master注册并申请资源（CPU Core 和Memory）；

*    2.Master根据SparkContext的资源申请要求和Worker心跳周期内报告的信息决定在哪个Worker上分配资源，然后在该Worker上获取资源，然后启动StandaloneExecutorBackend；

*    3.StandaloneExecutorBackend向SparkContext注册；

*    4.SparkContext将Applicaiton代码发送给StandaloneExecutorBackend；并且SparkContext解析Applicaiton代码，构建DAG图，并提交给DAG Scheduler分解成Stage（当碰到Action操作时，就会催生Job；每个Job中含有1个或多个Stage，Stage一般在获取外部数据和shuffle之前产生），然后以Stage（或者称为TaskSet）提交给Task Scheduler，Task Scheduler负责将Task分配到相应的Worker，最后提交给StandaloneExecutorBackend执行；

*    5.StandaloneExecutorBackend会建立Executor线程池，开始执行Task，并向SparkContext报告，直至Task完成。

*    6.所有Task完成后，SparkContext向Master注销，释放资源。

## Spark On Yarn
 YARN是一种统一资源管理机制，在其上面可以运行多套计算框架。目前的大数据技术世界，大多数公司除了使用Spark来进行数据计算，由于历史原因或者单方面业务处理的性能考虑而使用着其他的计算框架，比如MapReduce、Storm等计算框架。Spark基于此种情况开发了Spark on YARN的运行模式，由于借助了YARN良好的弹性资源管理机制，不仅部署Application更加方便，而且用户在YARN集群中运行的服务和Application的资源也完全隔离，更具实践应用价值的是YARN可以通过队列的方式，管理同时运行在集群中的多个服务。

Spark on YARN模式根据Driver在集群中的位置分为两种模式：一种是YARN-Client模式，另一种是YARN-Cluster（或称为YARN-Standalone模式）。
### 1：Yarn框架流程

   任何框架与YARN的结合，都必须遵循YARN的开发模式。在分析Spark on YARN的实现细节之前，有必要先分析一下YARN框架的一些基本原理。Yarn框架的基本流程如下：
![13-7](https://github.com/2415970940/save_img/raw/master/spark/13/13-7.png)   
其中，ResourceManager负责将集群的资源分配给各个应用使用，而资源分配和调度的基本单位是Container，其中封装了机器资源，如内存、CPU、磁盘和网络等，每个任务会被分配一个Container，该任务只能在该Container中执行，并使用该Container封装的资源。NodeManager是一个个的计算节点，主要负责启动Application所需的Container，监控资源（内存、CPU、磁盘和网络等）的使用情况并将之汇报给ResourceManager。ResourceManager与NodeManagers共同组成整个数据计算框架，ApplicationMaster与具体的Application相关，主要负责同ResourceManager协商以获取合适的Container，并跟踪这些Container的状态和监控其进度。

### 2：Yarn Client模式 
Yarn-Client模式中，Driver在客户端本地运行，这种模式可以使得Spark Application和客户端进行交互，因为Driver在客户端，所以可以通过webUI访问Driver的状态，默认是http://hadoop1:4040 访问，而YARN通过http:// hadoop1:8088访问。

  YARN-client的工作流程分为以下几个步骤：
 ![13-8](https://github.com/2415970940/save_img/raw/master/spark/13/13-8.png)

* (1).Spark Yarn Client向YARN的ResourceManager申请启动Application Master。同时在SparkContent初始化中将创建DAGScheduler和TASKScheduler等，由于我们选择的是Yarn-Client模式，程序会选择YarnClientClusterScheduler和YarnClientSchedulerBackend；

* (2).ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，与YARN-Cluster区别的是在该ApplicationMaster不运行SparkContext，只与SparkContext进行联系进行资源的分派；

* (3).Client中的SparkContext初始化完毕后，与ApplicationMaster建立通讯，向ResourceManager注册，根据任务信息向ResourceManager申请资源（Container）；

*  (4).一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向Client中的SparkContext注册并申请Task；

* (5).Client中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向Driver汇报运行的状态和进度，以让Client随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务；
*  (6).应用程序运行完成后，Client的SparkContext向ResourceManager申请注销并关闭自己

###  3：Spark Cluster模式

 在YARN-Cluster模式中，当用户向YARN中提交一个应用程序后，YARN将分两个阶段运行该应用程序：第一个阶段是把Spark的Driver作为一个ApplicationMaster在YARN集群中先启动；第二个阶段是由ApplicationMaster创建应用程序，然后为它向ResourceManager申请资源，并启动Executor来运行Task，同时监控它的整个运行过程，直到运行完成。

 YARN-cluster的工作流程分为以下几个步骤：
![13-9](https://github.com/2415970940/save_img/raw/master/spark/13/13-9.png)       

* (1).   Spark Yarn Client向YARN中提交应用程序，包括ApplicationMaster程序、启动ApplicationMaster的命令、需要在Executor中运行的程序等；

* (2).   ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，其中ApplicationMaster进行SparkContext等的初始化；

* (3).   ApplicationMaster向ResourceManager注册，这样用户可以直接通过ResourceManage查看应用程序的运行状态，然后它将采用轮询的方式通过RPC协议为各个任务申请资源，并监控它们的运行状态直到运行结束；

* (4).   一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向ApplicationMaster中的SparkContext注册并申请Task。这一点和Standalone模式一样，只不过SparkContext在Spark Application中初始化时，使用CoarseGrainedSchedulerBackend配合YarnClusterScheduler进行任务的调度，其中YarnClusterScheduler只是对TaskSchedulerImpl的一个简单包装，增加了对Executor的等待逻辑等；

* (5).   ApplicationMaster中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向ApplicationMaster汇报运行的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务；

* (6).   应用程序运行完成后，ApplicationMaster向ResourceManager申请注销并关闭自己。

### 4：Spark Client 和 Spark Cluster的区别
![13-10](https://github.com/2415970940/save_img/raw/master/spark/13/13-10.png)

理解YARN-Client和YARN-Cluster深层次的区别之前先清楚一个概念：Application Master。在YARN中，每个Application实例都有一个ApplicationMaster进程，它是Application启动的第一个容器。它负责和ResourceManager打交道并请求资源，获取资源之后告诉NodeManager为其启动Container。从深层次的含义讲YARN-Cluster和YARN-Client模式的区别其实就是ApplicationMaster进程的区别。

*   YARN-Cluster模式下，Driver运行在AM(Application Master)中，它负责向YARN申请资源，并监督作业的运行状况。当用户提交了作业之后，就可以关掉Client，作业会继续在YARN上运行，因而YARN-Cluster模式不适合运行交互类型的作业；

*  YARN-Client模式下，Application Master仅仅向YARN请求Executor，Client会和请求的Container通信来调度他们工作，也就是说Client不能离开。

补充：在standalone模式转为yarn模式，要在spark安装目录下改个配置
vi /usr/local/spark/conf/spark-env.sh
添加hadoop 
export HADOOP_HOME=/usr/local/hadoop

# SparkContext原理剖析
![13-11 SparkContext原理剖析](https://github.com/2415970940/save_img/raw/master/spark/13/13-11.png)
SparkContext.scala
```scala
...
// Create and start the scheduler
private[spark] var (schedulerBackend, taskScheduler) =
    SparkContext.createTaskScheduler(this, master)
	
@volatile private[spark] var dagScheduler: DAGScheduler = _ //DAGScheduler.scala
...	
//createTaskScheduler方法
private def createTaskScheduler(
      sc: SparkContext,
      master: String): (SchedulerBackend, TaskScheduler) = {
	  	master match {
		//standalone模式
			case SPARK_REGEX(sparkUrl) =>
				val scheduler = new TaskSchedulerImpl(sc)
				val masterUrls = sparkUrl.split(",").map("spark://" + _)
				val backend = new SparkDeploySchedulerBackend(scheduler, sc, masterUrls)
				scheduler.initialize(backend)
				(backend, scheduler)
				}
...				
//createTaskScheduler方法执行完后
taskScheduler.start() //TaskSchedulerImpl.scala的start（）方法
...
```


TaskSchedulerImpl.scala
```scala
/*TaskSchedulerImpl的实现
*1、底层通过操作一个SchedulerBackend，针对不同种类的cluster（standalone、yarn、mesos），调度task
*2、它也可以通过使用一个LocalBackend，并将isLocal参数设置为true，来在本地模式下工作
*3、它负责处理一些通用的逻辑，比如说决定多个job的顺序，启动推测任务执行
*4、客户端首先应用调用它的initialize（）方法和start（）方法，然后通过runTasks（）方法提交task sets
/
/**
 * Schedules tasks for multiple types of clusters by acting through a SchedulerBackend.
 * It can also work with a local setup by using a LocalBackend and setting isLocal to true.
 * It handles common logic, like determining a scheduling order across jobs, waking up to launch
 * speculative tasks, etc.
 *
 * Clients should first call initialize() and start(), then submit task sets through the
 * runTasks method.
 *
 * THREADING: SchedulerBackends and task-submitting clients can call this class from multiple
 * threads, so it needs locks in public API methods to maintain its state. In addition, some
 * SchedulerBackends synchronize on themselves when they want to send events here, and then
 * acquire a lock on us, so we need to make sure that we don't try to lock the backend while
 * we are holding a lock on ourselves.
 */
 
 ...
   def initialize(backend: SchedulerBackend) {
    this.backend = backend
    // temporarily set rootPool name to empty
    rootPool = new Pool("", schedulingMode, 0, 0)
    schedulableBuilder = {
      schedulingMode match {
        case SchedulingMode.FIFO =>
          new FIFOSchedulableBuilder(rootPool)
        case SchedulingMode.FAIR =>
          new FairSchedulableBuilder(rootPool, conf)
      }
    }
...	
 override def start() {
    backend.start()//SparkDeploySchedulerBackend.scala的start（）方法

    if (!isLocal && conf.getBoolean("spark.speculation", false)) {
      logInfo("Starting speculative execution thread")
      import sc.env.actorSystem.dispatcher
      sc.env.actorSystem.scheduler.schedule(SPECULATION_INTERVAL milliseconds,
            SPECULATION_INTERVAL milliseconds) {
        Utils.tryOrExit { checkSpeculatableTasks() }
      }
    }
  }
  ...
```

SparkDeploySchedulerBackend.scala
```scala
...
override def start() {
    super.start()
	...
	// ApplicationDescription非常重要，设置运行脚本的信息
	//Application需要多少cpu core，每个slave需要多少内存
	val appDesc = new ApplicationDescription(sc.appName, maxCores, sc.executorMemory, command,
      appUIAddress, sc.eventLogDir, sc.eventLogCodec)
	  //创建AppClient
	client = new AppClient(sc.env.actorSystem, masters, appDesc, this, conf)//AppClient.scala
    client.start()
	  ...
	}
...
```
AppClient.scala
```scala
/*它是一个接口
*它负责为application与spark集群通信
*它会接收spark master的url，以及一个ApplicationDescription，和一个集群事件的监听器，以及各种事件发生时，监听器的回调函数
*
*/
/**
 * Interface allowing applications to speak with a Spark deploy cluster. Takes a master URL,
 * an app description, and a listener for cluster events, and calls back the listener when various
 * events occur.
 *
 * @param masterUrls Each url should look like spark://host:port.
 */
 ...
 def start() {
    // Just launch an actor; it will call back into the listener.
    actor = actorSystem.actorOf(Props(new ClientActor))
  }
  ...
  //ClientActor是AppClient的内部类
  class ClientActor extends Actor with ActorLogReceive with Logging{
  ...
     def tryRegisterAllMasters() {
      for (masterAkkaUrl <- masterAkkaUrls) {
        logInfo("Connecting to master " + masterAkkaUrl + "...")
        val actor = context.actorSelection(masterAkkaUrl)
        actor ! RegisterApplication(appDescription)
      }
    }

    def registerWithMaster() {
      tryRegisterAllMasters()
      import context.dispatcher
      var retries = 0
      registrationRetryTimer = Some {
        context.system.scheduler.schedule(REGISTRATION_TIMEOUT, REGISTRATION_TIMEOUT) {
          Utils.tryOrExit {
            retries += 1
            if (registered) {
              registrationRetryTimer.foreach(_.cancel())
            } else if (retries >= REGISTRATION_RETRIES) {
              markDead("All masters are unresponsive! Giving up.")
            } else {
              tryRegisterAllMasters()
            }
          }
        }
      }
  }
  ...
```

/DAGScheduler.scala

``` scala
/*实现了面向stage的调度机制的高层次的调度层，它会为每个job计算一个stage的DGA（有向无环图），追踪RDD和stage的输出是否被物化（物化就是说写入磁盘或内存地方），并且寻找最小调度机制（最优）来运行job
*它会将stage作为tasksets提交到底层的TaskSchedulerImpl，来在集群上运行他们（task）。
*
*除了处理stage的DAG，它还负责决定运行每个task的最佳位置，基于当前缓存状态，将这些最佳位置提交给底层的TaskSchedulerImpl，此外，它还会处理由于shuffle输出文件丢失导致的失败，在这种情况下，旧的stage被重新提交。
一个stage内部的失败，如果不是由于shuffle输出文件丢失导致的失败，会被TaskScheduler处理，它会多次重试每个task，直到最后，会取消这个stage。
*/
/**
 * The high-level scheduling layer that implements stage-oriented scheduling. It computes a DAG of
 * stages for each job, keeps track of which RDDs and stage outputs are materialized, and finds a
 * minimal schedule to run the job. It then submits stages as TaskSets to an underlying
 * TaskScheduler implementation that runs them on the cluster.
 *
 * In addition to coming up with a DAG of stages, this class also determines the preferred
 * locations to run each task on, based on the current cache status, and passes these to the
 * low-level TaskScheduler. Furthermore, it handles failures due to shuffle output files being
 * lost, in which case old stages may need to be resubmitted. Failures *within* a stage that are
 * not caused by shuffle file loss are handled by the TaskScheduler, which will retry each task
 * a small number of times before cancelling the whole stage.
 *
 * Here's a checklist to use when making or reviewing changes to this class:
 *
 *  - When adding a new data structure, update `DAGSchedulerSuite.assertDataStructuresEmpty` to
 *    include the new structure. This will help to catch memory leaks.
 */
```
# Master原理剖析和源码分析

1、主备切换机制原理剖析
2、注册机制原理剖析与源码分析
3、状态改变机制源码分析
4、资源调度机制源码分析（两种资源调度算法）
