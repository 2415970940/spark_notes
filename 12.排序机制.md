# wordcount排序机制
1.文本文件内每个单词统计其出现的次数
2.每个单词出现数量，进行降序排序

``` java
package cn.spark.study.core;

import java.util.Arrays;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.api.java.function.PairFunction;
import org.apache.spark.api.java.function.VoidFunction;

import scala.Tuple2;

/*
 * wordcount的排序
 */
public class SortWordCount {
	public static void main(String[] args) {
		SparkConf conf = new SparkConf()
				.setAppName("SortWordCount")
				.setMaster("local");
		JavaSparkContext sc = new JavaSparkContext(conf);
		
		JavaRDD<String> lines = sc.textFile("H://spark//spark-study//spark.txt");
		JavaRDD<String> words = lines.flatMap(new FlatMapFunction<String, String>() {

			@Override
			public Iterable<String> call(String line) throws Exception {
				// TODO Auto-generated method stub
				return Arrays.asList(line.split(" "));
			}
		});
		JavaPairRDD<String,Integer> wordPairs=words.mapToPair(new PairFunction<String, String, Integer>() {

			@Override
			public Tuple2<String, Integer> call(String word) throws Exception {
				// TODO Auto-generated method stub
				return new Tuple2<String, Integer>(word,1);
			}
		});
		JavaPairRDD<String, Integer> wordcounts=wordPairs.reduceByKey(new Function2<Integer, Integer, Integer>() {
			
			@Override
			public Integer call(Integer v1, Integer v2) throws Exception {
				// TODO Auto-generated method stub
				return v1+v2;
			}
		});
		//将RDD中的（word，5）转换成（5，word），然后在进行排序
		//反转映射
		JavaPairRDD<Integer, String> countwords=wordcounts.mapToPair(new PairFunction<Tuple2<String,Integer>,Integer, String>() {

			@Override
			public Tuple2<Integer, String> call(Tuple2<String, Integer> t) throws Exception {
				// TODO Auto-generated method stub
				return new Tuple2<Integer, String>(t._2,t._1);
			}
		});
		//反转映射
		JavaPairRDD<Integer, String> sortcountwords=countwords.sortByKey(false);
		
		JavaPairRDD<String, Integer> sortwordcounts=sortcountwords.mapToPair(new PairFunction<Tuple2<Integer,String>, String,Integer>() {

			@Override
			public Tuple2<String, Integer> call(Tuple2<Integer, String> t) throws Exception {
				// TODO Auto-generated method stub
				return new Tuple2<String, Integer>(t._2,t._1);
			}
		});
		
		sortwordcounts.foreach(new VoidFunction<Tuple2<String,Integer>>() {
			
			@Override
			public void call(Tuple2<String, Integer> wordcount) throws Exception {
				// TODO Auto-generated method stub
				System.out.println(wordcount._1+":"+wordcount._2);
			}
		});
		
		sc.close();
		
	}

}

```
scala
```scala
package cn.spark.study.core

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

object SortWordCount {
    def main(args: Array[String]): Unit = {
    val conf=new SparkConf().setAppName("WordCountLocal").setMaster("local")
    val sc = new SparkContext(conf)
    
    val lines=sc.textFile("H://spark//spark-study//spark.txt", 1)
    val words = lines.flatMap { line => line.split(" ") }
    val pairs=words.map{word=>(word,1)}
    val wordcounts = pairs.reduceByKey{_+_}
    
    val countwords=wordcounts.map{wordcount=>(wordcount._2,wordcount._1)}
    val sortcountwords=countwords.sortByKey(false)
    val sortwordcounts=sortcountwords.map{sortcountword=>(sortcountword._2,sortcountword._1)}
    
    sortwordcounts.foreach(wordcount=>{println(wordcount._1+":"+wordcount._2)})
    sc.stop()
  }
}
```
# 二次排序
java中首先实现SecondarySortKey

``` java
package cn.spark.study.core;

import java.io.Serializable;

import scala.math.Ordered;

/*
 *  自定义二次排序key
 * 
 */
public class SecondarySortKey implements Ordered<SecondarySortKey>,Serializable{

	private static final long serialVersionUID = 1L;
	
	//自定义需要排序的列
	private int first;
	private int second;
	//构造方法
	
	//排序的列，提供getter,setter,hashCode,equals方法

	public int getFirst() {
		return first;
	}

	public SecondarySortKey(int first, int second) {
		super();
		this.first = first;
		this.second = second;
	}

	public void setFirst(int first) {
		this.first = first;
	}

	public int getSecond() {
		return second;
	}

	public void setSecond(int second) {
		this.second = second;
	}
	

	@Override
	public int hashCode() {
		final int prime = 31;
		int result = 1;
		result = prime * result + first;
		result = prime * result + second;
		return result;
	}

	@Override
	public boolean equals(Object obj) {
		if (this == obj)
			return true;
		if (obj == null)
			return false;
		if (getClass() != obj.getClass())
			return false;
		SecondarySortKey other = (SecondarySortKey) obj;
		if (first != other.first)
			return false;
		if (second != other.second)
			return false;
		return true;
	}
	

	//实现比较算法和逻辑
	@Override
	public boolean $greater(SecondarySortKey other) {
		if(this.first>other.getFirst()) {
			return true;
		}else if (this.first==other.getFirst() && this.second>other.getSecond()) {
			return true;
		}
					
		return false;
	}

	@Override
	public boolean $greater$eq(SecondarySortKey other) {
		if (this.$greater(other)) {
			return true;
		} else if(this.first==other.getFirst() && this.second==other.getSecond()){
			return true;
		}
		return false;
	}

	@Override
	public boolean $less(SecondarySortKey other) {
		if(this.first<other.getFirst()) {
			return true;
		}else if (this.first==other.getFirst() && this.second<other.getSecond()) {
			return true;
		}
		return false;
	}

	@Override
	public boolean $less$eq(SecondarySortKey other) {
		if (this.$less(other)) {
			return true;
		} else if(this.first==other.getFirst() && this.second==other.getSecond()){
			return true;
		}
		return false;
	}

	@Override
	public int compare(SecondarySortKey other) {
		if (this.first-other.getFirst()!=0) {
			return this.first-other.getFirst();
		} else {
			return this.second-other.getSecond();
		}
	}

	@Override
	public int compareTo(SecondarySortKey other) {
		if (this.first-other.getFirst()!=0) {
			return this.first-other.getFirst();
		} else {
			return this.second-other.getSecond();
		}
	}

}

```

```java
package cn.spark.study.core;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.PairFunction;
import org.apache.spark.api.java.function.VoidFunction;

import scala.Tuple2;
/*
 * 二次排序
 * 1.实现自定义的key。要实现Ordered接口和Serializable接口，在key中实现自己对多个列的排序算法
 * 2.将包含文本的RDD，映射成key为自定义key，value为文本的JavaPairRDD
 * 3.使用sortByKey算子按照自定义的key进行排序
 * 4.再次映射，剔除自定义key，保留文本行
 * 
 * 
 */
public class SecondarySort {

	public static void main(String[] args) {
		SparkConf conf =new SparkConf()
				.setAppName("SecondarySort")
				.setMaster("local");
		JavaSparkContext sc = new JavaSparkContext(conf);
		
		JavaRDD<String> lines=sc.textFile("H://spark//spark-study//sort.txt");
		
		JavaPairRDD<SecondarySortKey, String> pairs=lines.mapToPair(new PairFunction<String, SecondarySortKey, String>() {

			@Override
			public Tuple2<SecondarySortKey, String> call(String line) throws Exception {
				String[] linepair=line.split(" ");
				SecondarySortKey key=new SecondarySortKey(
						Integer.valueOf(linepair[0]), 
						Integer.valueOf(linepair[1]));
				return new Tuple2<SecondarySortKey, String>(key, line);
			}
		});
		
		JavaPairRDD<SecondarySortKey, String> sortPairs=pairs.sortByKey();
		
		JavaRDD<String> sortlines=sortPairs.map(new Function<Tuple2<SecondarySortKey,String>, String>() {

			@Override
			public String call(Tuple2<SecondarySortKey, String> line) throws Exception {
				return line._2;
			}
		});
		
		sortlines.foreach(new VoidFunction<String>() {
			
			@Override
			public void call(String line) throws Exception {
				System.out.println(line);
			}
		});
		sc.close();

	}

}

```

scala

``` scala
package cn.spark.study.core

class SecondSortKey(val first:Int,val second:Int) extends Ordered[SecondSortKey] with Serializable {
  def compare(that: SecondSortKey): Int = {
    if (this.first-that.first != 0) {
      return this.first-that.first      
    }else {
      return this.second-that.second
    }
  }
}
```

```scala
package cn.spark.study.core

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

object SecondarySort {
      def main(args: Array[String]): Unit = {
    val conf=new SparkConf().setAppName("WordCountLocal").setMaster("local")
    val sc = new SparkContext(conf)
    
    val lines=sc.textFile("H://spark//spark-study//sort.txt", 1)

    val pairs = lines.map { line => (
        new SecondSortKey(line.split(" ")(0).toInt,line.split(" ")(1).toInt),
        line) }
    val sortpairs = pairs.sortByKey()
    val sortlines=sortpairs.map{sortpair=>sortpair._2}
    
    sortlines.foreach { line => println(line) }

    
  }
}
```