[toc]
# 1.用Java开发wordcount程序
## 1.1配置maven环境
```xml
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
  <modelVersion>4.0.0</modelVersion>

  <groupId>cn.spark</groupId>
  <artifactId>spark-study-java</artifactId>
  <version>0.0.1-SNAPSHOT</version>
  <packaging>jar</packaging>

  <name>spark-study-java</name>
  <url>http://maven.apache.org</url>

  <properties>
    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
  </properties>

  <dependencies>
    <dependency>
      <groupId>junit</groupId>
      <artifactId>junit</artifactId>
      <version>3.8.1</version>
      <scope>test</scope>
    </dependency>
    <dependency>
    	<groupId>org.apache.spark</groupId>
    	<artifactId>spark-core_2.10</artifactId>
    	<version>1.3.0</version>
	</dependency>
	<dependency>
    	<groupId>org.apache.spark</groupId>
    	<artifactId>spark-sql_2.10</artifactId>
    	<version>1.3.0</version>
	</dependency>
	<dependency>
    	<groupId>org.apache.spark</groupId>
    	<artifactId>spark-hive_2.10</artifactId>
    	<version>1.3.0</version>
    	<scope>provided</scope>
	</dependency>
	<dependency>
    	<groupId>org.apache.spark</groupId>
    	<artifactId>spark-streaming_2.10</artifactId>
    	<version>1.3.0</version>
    	<scope>provided</scope>
	</dependency>
	<dependency>
	    <groupId>org.apache.hadoop</groupId>
	    <artifactId>hadoop-client</artifactId>
	    <version>2.4.1</version>
	</dependency>
	<dependency>
	    <groupId>org.apache.spark</groupId>
	    <artifactId>spark-streaming-kafka_2.10</artifactId>
	    <version>1.3.0</version>
	</dependency>
	
  </dependencies>


	
	<build>
		<sourceDirectory>src/main/scala</sourceDirectory>
    	<testSourceDirectory>src/test/scala</testSourceDirectory>
    	<plugins>
      		<plugin>
      			<artifactId>maven-assembly-plugin</artifactId>
      			<configuration>
      				<descriptorRefs>
      					<descriptorRef>jar-with-dependencies</descriptorRef>
      				</descriptorRefs>
      				<archive>
      					<manifest>
      						<mainClass></mainClass>
      					</manifest>
      				</archive>
      			</configuration>
      			<executions>
      				<execution>
      					<id>make-assembly</id>
      					<phase>package</phase>
      					<goals>
      						<goal>single</goal>
      					</goals>
      				</execution>
      			</executions>
      		</plugin>
	        <plugin>
	            <groupId>org.codehaus.mojo</groupId>
	            <artifactId>exec-maven-plugin</artifactId>
	            <version>1.2.1</version>
	            <executions>
	                <execution>
	                    <goals>
	                        <goal>exec</goal>
	                    </goals>
	                </execution>
	            </executions>
	            <configuration>
	                <executable>java</executable>
	                <includeProjectDependencies>true</includeProjectDependencies>
	                <includePluginDependencies>false</includePluginDependencies>
	                <classpathScope>compile</classpathScope>
	                <mainClass>cn.spark.study.App</mainClass>
	            </configuration>
	        </plugin>
	        <plugin>
	            <groupId>org.apache.maven.plugins</groupId>
	            <artifactId>maven-compiler-plugin</artifactId>	
	            <configuration>
	                <source>1.6</source>
	                <target>1.6</target>
	            </configuration>
        	</plugin>   		
      	</plugins>	
	</build>

</project>
```
## 1.2如何进行本地测试
```scala
package cn.spark.study.core;

import java.util.Arrays;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.api.java.function.PairFunction;
import org.apache.spark.api.java.function.VoidFunction;

import scala.Tuple2;

/*
  * 本地测试程序
 * 
 */

public class WordCountLocal {
	public static void main(String[] args) {
	//	编写spark应用程序
	//第一步：创建sparkConf对象，设置Spark应用的配置信息
	//使用setMaster()可以设置spark应用程序要连接的spark集群的master节点的url
	//但是如果设置为local则代表在本地运行
		SparkConf conf = new SparkConf()
				.setAppName("WordCountLocal")
				.setMaster("local");
	//第二步:创建JavaSparkContext对象
		//在spark中,SparkContext是spark所有功能的一个入口,你无论是用Java、scala,甚至是python编写
		//都必须要有一个SparkContext,它的主要作用,包括初始化spark应用程序所需的一些核心组件,包括
		//调度器(DAGSchedule、TaskScheduler),还会去到Spark Master节点上进行注册,等等

		//一句话,SparkContext,是spark应用中,可以说是最最重要的一个对象

		//但是呢,在spark中,编写不同类型的spark应用程序,使用的SparkContext是不同的
		//如果使用scala,使用的就是原生的SparkContext对象
		//但是如果使用Java,那么就是JavaSparkContext对象
		//如果是开发spark SQL程序,那么就是SQLContext、HiveContext
		//如果是开发spark Streaming程序,那么就是他独有的SparkContext
		
		JavaSparkContext sc = new JavaSparkContext(conf);
	//第三步要针对输入源（hdfs文件，本地文件等等），创建一个初始的RDD
		//输入源中的数据会打散,分配到RDD的每个partition中,从而形成一个初始的分布式的数据集
		//SparkContext中,用于根据文件类型的输入源创建RDD的方法,叫做textFile()方法
		//在Java中，创建的普通RDD，都叫做JavaRDD
		//如果是hdfs或者本地文件创建的RDD，每个元素相当文件的一行，也就是元素概念
		
		JavaRDD<String> lines = sc.textFile("spark.txt");
	//第四步：对初始化RDD进行transformation操作，也就是一些计算操作
		//通常操作会通过创建function,并配合RDD的map、flatmap等算子来执行
		//function,通常,如果比较简单,则创建指定function的匿名内部类
		//但是如果function比较复杂,则会单独创建一个类，作为实现这个function接口的类		
		
		//将每行拆成一个单词
		//FlatMapFunction有两个泛型参数，分别是输入和输出类型
		//flatmap算子的作用,其实就是,将RDD的一个元素,给拆分成一个或多个元素

	
		JavaRDD<String> words =lines.flatMap(new FlatMapFunction<String, String>() {

			/**
			 * 
			 */
			private static final long serialVersionUID = 1L;

			@Override
			public Iterable<String> call(String line) throws Exception {
				// TODO Auto-generated method stub
				return Arrays.asList(line.split(" "));
			}
		});
		//映射为（单词，1）
		//mapToPair 就是将其映射为（k,v）,Tuple2类型。这里的Tuple2就是scala类型
		//mapToPair要与PairFunction匹配，第一泛型是输入，第二个和第三个是输出泛型，Tuple2
		//JavaPairRDD两个泛型参数，对应Tuple2
		JavaPairRDD<String, Integer> pairs = words.mapToPair(new PairFunction<String, String, Integer>() {

			/**
			 * 
			 */
			private static final long serialVersionUID = 1L;

			@Override
			public Tuple2<String, Integer> call(String word) throws Exception {
				// TODO Auto-generated method stub
				return new Tuple2<String, Integer>(word, 1);
			}
			
		});
		//接着，以单词为key，统计出现的次数
		//使用reduceByKey算子，对每个key对应的value进行reduce操作
		JavaPairRDD<String, Integer> wordCount=pairs.reduceByKey(new Function2<Integer, Integer, Integer>() {
			
			/**
			 * 
			 */
			private static final long serialVersionUID = 1L;

			@Override
			public Integer call(Integer v1, Integer v2) throws Exception {
				// TODO Auto-generated method stub
				return v1+v2;
			}
		});
		//到此为止，通过几个spark算子操作，计算出单词出现次数		
		//之前是我们使用的flatMap，mapToPair,reduceByKey操作，都叫做transformation操作
		//transformation和action组合，才能运行spark应用
		//接着，action操作，比如foreach来触发程序执行
		wordCount.foreach(new VoidFunction<Tuple2<String,Integer>>() {
			
			/**
			 * 
			 */
			private static final long serialVersionUID = 1L;

			@Override
			public void call(Tuple2<String, Integer> wordCount) throws Exception {
				// TODO Auto-generated method stub
				System.out.println(wordCount._1+":"+wordCount._2);
			}
		});
		sc.close();
	}
}
```
## 1.3如何使用spark-submit提交到spark集群进行执行
(spark-submi常用参数说明,spark-submit其实就类似于hadoop的hadoop.jar命令)
1.将spark.txt上传到spark1的：/usr/local/下
 hadoop fs -put spark.txt /spark.txt
 查看方式：浏览器上spark1:50070下utilities-》Browse the file system
 2.根据pom.xml里的配置maven插件，对spark进行打包
 打包过程：右键项目-》run as-》run configurations-》Maven Build右击-》new-》Name:(起名字，如spark-study-java)->点击
 browse workspace选择项目-》Goals输入clean package-Run
 最后生成spark-study-java-0.0.1-SNAPSHOT-jar-with-dependencies.jar
3. 将jar包上传到spark集群上，（我的位置root@spark1:/usr/local/spark-study/java/）
4. 编写spark-submit脚本，然后执行脚本，提交到集群上
wordcount.sh
```sh
/usr/local/spark/bin/spark-submit \
--class cn.spark.study.core.WorkCountCluster \
--num-executors 3 \
--driver-memory 100m \
--executor-memory 100m \
--executor-cores 3 \
/usr/local/spark-study/java/spark-study-java-0.0.1-SNAPSHOT-jar-with-dependencies.jar \
```
给sh文件权限，chmod 777 wordcount.sh
5.执行./wordcount.sh
浏览器可以查看（运行时可以看）http://spark1:4040

# 2.用Scala开发wordcount程序
## 2.1下载scala ide for eclipse
## 2.2在Java Build Path中,添加spark依赖包
(如果与scala ide for eclpse原生的scala版本
发生冲突,则移除原生的scala,重新配置scala compiler)
* 在spark安装文件spark-1.3.0-bin-hadoop2.4.tgz中，目录下lib下的
spark-assembly-1.3.0-hadoop2.4.0.jar添加到eclipse  libraries
* 建立包cn.spark.study.core,创建类WordCount

``` scala
package cn.spark.study.core

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

object WordCount {
  def main(args: Array[String]): Unit = {
    val conf=new SparkConf().setAppName("WordCount")
    val sc = new SparkContext(conf)
    
    val lines=sc.textFile("hdfs://spark1:9000/spark.text", 1)
    val words = lines.flatMap { line => line.split(" ") }
    val pairs=words.map{word=>(word,1)}
    val wordcounts = pairs.reduceByKey{_+_}
    
    wordcounts.foreach(wordcount=>{println(wordcount._1+":"+wordcount._2)})
  }
}
```
一些坑：点击项目然后右键选中properties=》scala compiler=>scala installation选中Latest 2.10 bundle  后面jvm-1.6
## 2.3用添加export导出scala spark工程
将导出的jar包上传spark集群，然后执行其脚本


# 3.用spark-shell开发wordcount程序
## 3.1常用于简单的测试

# 4.wordcount程序原理剖析
![8-1 wordcount程序原理剖析图](）

# 5.spark架构原理
1.Drive
2.Master
3.Worker
4.Executor
5.Task
![8-2 spark构架原理]()
![8-3 spark执行顺序]()