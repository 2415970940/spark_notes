[toc]
# 1.用Java开发wordcount程序
## 1.1配置maven环境
```xml
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
  <modelVersion>4.0.0</modelVersion>

  <groupId>cn.spark</groupId>
  <artifactId>spark-study-java</artifactId>
  <version>0.0.1-SNAPSHOT</version>
  <packaging>jar</packaging>

  <name>spark-study-java</name>
  <url>http://maven.apache.org</url>

  <properties>
    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
  </properties>

  <dependencies>
    <dependency>
      <groupId>junit</groupId>
      <artifactId>junit</artifactId>
      <version>3.8.1</version>
      <scope>test</scope>
    </dependency>
    <dependency>
    	<groupId>org.apache.spark</groupId>
    	<artifactId>spark-core_2.10</artifactId>
    	<version>1.3.0</version>
	</dependency>
	<dependency>
    	<groupId>org.apache.spark</groupId>
    	<artifactId>spark-sql_2.10</artifactId>
    	<version>1.3.0</version>
	</dependency>
	<dependency>
    	<groupId>org.apache.spark</groupId>
    	<artifactId>spark-hive_2.10</artifactId>
    	<version>1.3.0</version>
    	<scope>provided</scope>
	</dependency>
	<dependency>
    	<groupId>org.apache.spark</groupId>
    	<artifactId>spark-streaming_2.10</artifactId>
    	<version>1.3.0</version>
    	<scope>provided</scope>
	</dependency>
	<dependency>
	    <groupId>org.apache.hadoop</groupId>
	    <artifactId>hadoop-client</artifactId>
	    <version>2.4.1</version>
	</dependency>
	<dependency>
	    <groupId>org.apache.spark</groupId>
	    <artifactId>spark-streaming-kafka_2.10</artifactId>
	    <version>1.3.0</version>
	</dependency>
	
  </dependencies>


	
	<build>
		<sourceDirectory>src/main/scala</sourceDirectory>
    	<testSourceDirectory>src/test/scala</testSourceDirectory>
    	<plugins>
      		<plugin>
      			<artifactId>maven-assembly-plugin</artifactId>
      			<configuration>
      				<descriptorRefs>
      					<descriptorRef>jar-with-dependencies</descriptorRef>
      				</descriptorRefs>
      				<archive>
      					<manifest>
      						<mainClass></mainClass>
      					</manifest>
      				</archive>
      			</configuration>
      			<executions>
      				<execution>
      					<id>make-assembly</id>
      					<phase>package</phase>
      					<goals>
      						<goal>single</goal>
      					</goals>
      				</execution>
      			</executions>
      		</plugin>
	        <plugin>
	            <groupId>org.codehaus.mojo</groupId>
	            <artifactId>exec-maven-plugin</artifactId>
	            <version>1.2.1</version>
	            <executions>
	                <execution>
	                    <goals>
	                        <goal>exec</goal>
	                    </goals>
	                </execution>
	            </executions>
	            <configuration>
	                <executable>java</executable>
	                <includeProjectDependencies>true</includeProjectDependencies>
	                <includePluginDependencies>false</includePluginDependencies>
	                <classpathScope>compile</classpathScope>
	                <mainClass>cn.spark.study.App</mainClass>
	            </configuration>
	        </plugin>
	        <plugin>
	            <groupId>org.apache.maven.plugins</groupId>
	            <artifactId>maven-compiler-plugin</artifactId>	
	            <configuration>
	                <source>1.6</source>
	                <target>1.6</target>
	            </configuration>
        	</plugin>   		
      	</plugins>	
	</build>

</project>
```
## 1.2如何进行本地测试
## 1.3如何使用spark-submit提交到spark集群进行执行
(spark-submi常用参数说明,spark-submit其实就类似于hadoop的hadoop.jar命令)

# 2.用Scala开发wordcount程序
## 2.1下载scala ide for eclipse
## 2.2在Java Build Path中,添加spark依赖包
(如果与scala ide for eclpse原生的scala版本
发生冲突,则移除原生的scala,重新配置scala compiler)
## 2.3用export导出scala spark工程

# 3.用spark-shell开发wordcount程序
## 3.1常用于简单的测试
